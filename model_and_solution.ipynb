{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9988c05e-7257-4135-abf3-4432d661b1ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Подготовка признаков, обучение и тестирование модели\n",
    "\n",
    "Здесь предсталены основные этапы проекта: подготовка признаков, обучение модели и тестирование ее работы. \n",
    "\n",
    "Мы будем использовать набор данных, предоставленный компанией \"Союзмультфильм\", для классификации видеоклипов по соответствующим проектам.\n",
    "\n",
    "Конечная цель - максимизировать метрику F1 MACRO.\n",
    "\n",
    "## 1. Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c8bcafc-fb9b-46e9-8fba-c0c9171ec517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffd7b88-c71f-45f1-aa9a-10a2038ebbb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Пояснение\n",
    "Используем популярные библиотеки, такие как `pandas` для работы с данными, `scikit-learn` для машинного обучения и `nltk` для обработки естественного языка. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68096588-3ce8-4d1d-9b63-67445c6e1993",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Загрузка данных и ресурсов \n",
    "\n",
    "### Загрузка стоп-слов и инициализация лемматизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bbc117a-2ea5-4d7c-9e60-4ca35ff34e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/karpov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/karpov/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Загрузка ресурсов NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Лемматизация и стоп-слова\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "stop_words.update(stopwords.words('english'))  # Добавляем стоп-слова для английского"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81231d1f-a035-4fbf-ad9f-0c2e37e14810",
   "metadata": {},
   "source": [
    "### Загрузка данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aedc560e-985d-47cc-a35e-b713acb4a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "train = pd.read_csv('train.csv', lineterminator='\\n')\n",
    "test = pd.read_csv('test.csv', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c3bdf9-026e-4520-88cd-e0a79ce7373c",
   "metadata": {},
   "source": [
    "**Пояснение:** \n",
    "- `lineterminator='\\n'` используется для корректной обработки новых строк."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6388d4-9df9-4b1a-9fbb-0bde20eec542",
   "metadata": {},
   "source": [
    "## 3. Предварительная обработка данных \n",
    "\n",
    "### Функция предварительной обработки текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfe56d77-d963-4a28-8956-f9d0bf8ac756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предварительная обработка текста\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())  # Убираем лишние символы и приводим к нижнему регистру\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Лемматизация и удаление стоп-слов\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af3a41f-1f4b-48de-9ebb-4a25fbcacdd2",
   "metadata": {},
   "source": [
    "### Применение предобработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "719a4f61-f467-43fa-89b8-dd3b7edb370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применение предобработки к тексту\n",
    "train['text'] = train['text'].apply(lambda x: preprocess_text(str(x)))\n",
    "test['text'] = test['text'].apply(lambda x: preprocess_text(str(x)))\n",
    "\n",
    "# Предобработка 'reel_name'\n",
    "train['reel_name'] = train['reel_name'].apply(lambda x: preprocess_text(str(x)))\n",
    "test['reel_name'] = test['reel_name'].apply(lambda x: preprocess_text(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a177e6-6a16-40cb-9003-55834d7b9c6c",
   "metadata": {},
   "source": [
    "**Объяснение:** \n",
    "- Предварительная обработка текста включает в себя очистку, токенизацию, лемматизацию и удаление стоп-слов из текста. \n",
    "- И обучающие, и тестовые данные подвергаются одинаковой предварительной обработке, чтобы обеспечить согласованность."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b224ac-77de-47b1-9e77-b89f56e100d3",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "### Извлечение именованных сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23bc5de9-78c9-45fb-8415-166b1bc1517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для извлечения именованных сущностей с использованием регулярных выражений\n",
    "def extract_named_entities(text):\n",
    "    # Ищем слова, которые начинаются с заглавной буквы\n",
    "    names = re.findall(r'\\b[A-ZА-ЯЁ][a-zа-яё]*\\b', text)\n",
    "    return ' '.join(names) if names else 'unknown'\n",
    "\n",
    "# Извлечение именованных сущностей\n",
    "train['names_str'] = train['text'].apply(lambda x: extract_named_entities(str(x)))\n",
    "test['names_str'] = test['text'].apply(lambda x: extract_named_entities(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f7a574-23e4-48b9-a53f-c5f3ccb209ed",
   "metadata": {},
   "source": [
    "**Пояснение:** \n",
    "- Здесь мы извлекаем именованные сущности из текста с помощью регулярных выражений, предполагая, что слова, начинающиеся с заглавных букв, являются потенциальными именованными сущностями."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e292cc1-6e84-47aa-8c71-5661367a7e4a",
   "metadata": {},
   "source": [
    "## 5. Векторизация с помощью TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b944ffec-98b9-42e2-a9e2-5c3deaaa1661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Векторизация текста (TF-IDF) с использованием униграмм\n",
    "tfidf_text = TfidfVectorizer(max_features=10000, ngram_range=(1, 1))\n",
    "X_text = tfidf_text.fit_transform(train['text'])\n",
    "\n",
    "# Векторизация reel_name\n",
    "tfidf_reel = TfidfVectorizer(max_features=10000, ngram_range=(1, 1))\n",
    "X_reel = tfidf_reel.fit_transform(train['reel_name'])\n",
    "\n",
    "# Векторизация имен\n",
    "tfidf_names = TfidfVectorizer(max_features=5000, ngram_range=(1, 1))\n",
    "X_names = tfidf_names.fit_transform(train['names_str'])\n",
    "\n",
    "# Объединение признаков\n",
    "X = hstack([X_text, X_reel, X_names])\n",
    "y = train['cartoon']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819a9f63-758b-4c34-baee-63e9ec1020cc",
   "metadata": {},
   "source": [
    "**Пояснение:**\n",
    "- Для преобразования текстовых данных в векторы мы используем **TF-IDF**. \n",
    "- Текст, название ролика и извлеченные имена векторизуются отдельно, а затем объединяются в единый набор признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54228a9b-78d8-4ec6-ac4f-d096355fdad1",
   "metadata": {},
   "source": [
    "## 6. Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edad9303-c8e7-4c7e-ad39-f329de07a3b0",
   "metadata": {},
   "source": [
    "### Настройка и обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf219fab-a26f-4927-8c2c-79a849a2a5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Стратифицированная кросс-валидация с 3 разбиениями\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Определение модели RandomForestClassifier без балансировки классов\n",
    "model = RandomForestClassifier(max_features='sqrt',\n",
    "                               min_samples_split=2,\n",
    "                               random_state=42)\n",
    "\n",
    "# Обучение модели на всех данных\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6ee167-35f0-4c2e-aeb0-346f7d7f4dac",
   "metadata": {},
   "source": [
    "**Пояснение:**\n",
    "- В качестве модели мы используем **RandomForestClassifier** со стратифицированной кросс-валидацией для устранения дисбаланса классов. \n",
    "- Модель обучается на всех комбинированных признаках."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20471a20-1d40-4f75-95b5-927c61406d9a",
   "metadata": {},
   "source": [
    "## 7. Тестирование модели\n",
    "\n",
    "### Прогнозирование на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2fd6d15-aa5a-4c60-b002-5153cfef966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Прогнозирование на тестовых данных\n",
    "X_test_text = tfidf_text.transform(test['text'])\n",
    "X_test_reel = tfidf_reel.transform(test['reel_name'])\n",
    "X_test_names = tfidf_names.transform(test['names_str'])\n",
    "\n",
    "X_test = hstack([X_test_text, X_test_reel, X_test_names])\n",
    "\n",
    "test_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c6224d-6722-4ec7-9185-f9006070a6f7",
   "metadata": {},
   "source": [
    "### Создаём файл для kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "467623cb-2a75-416c-95e8-650fb38e49bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание файла submission\n",
    "submission = pd.DataFrame({\n",
    "    'yt_reel_id': test['yt_reel_id'],\n",
    "    'cartoon': test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission_t1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293983f-230c-41e1-9a9a-8d6a04c2afcf",
   "metadata": {},
   "source": [
    "## Резюме\n",
    "В этой тетрадке рассмотрены основные части проекта: \n",
    "1. **Подготовка признаков** - включая предварительную обработку текста и извлечение признаков. \n",
    "2. **Обучение модели** - использование RandomForestClassifier. \n",
    "3. **Тестирование и сохранение результатов** - составление прогнозов на тестовом наборе и сохранение результатов.\n",
    "\n",
    "Основной целью было создание надежного пайплайна извлечения признаков и обучение модели, которая могла бы точно классифицировать видеоклипы по различным проектам, максимизируя метрику F1 macro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36faa50-2b00-4c00-accf-9af6b8a99341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
